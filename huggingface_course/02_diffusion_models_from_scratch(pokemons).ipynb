{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shani1610/learning-diffusion-models/blob/main/huggingface_course/02_diffusion_models_from_scratch(pokemons).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPTzwfAgYWGi"
      },
      "source": [
        "# Diffusion Models from Scratch\n",
        "\n",
        "Sometimes it is helpful to consider the simplest possible version of something to better understand how it works. We're going to try that in this notebook, beginning with a 'toy' diffusion model to see how the different pieces work, and then examining how they differ from a more complex implementation.\n",
        "\n",
        "We will look at\n",
        "- The corruption process (adding noise to data)\n",
        "- What a UNet is, and how to implement an extremely minimal one from scratch\n",
        "- Diffusion model training\n",
        "- Sampling theory\n",
        "\n",
        "Then we'll compare our versions with the diffusers DDPM implementation, exploring improvements over our mini UNet\n",
        "\n",
        "It is also worth noting that most of the code here is for illustrative purposes, and I wouldn't recommend directly adopting any of it for your own work (unless you're just trying improve on the examples shown here for learning purposes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mr5W7P-Ybhq"
      },
      "source": [
        "## Setup and Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zxnsqe0MQ0m4"
      },
      "outputs": [],
      "source": [
        "%pip install -q diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "46978020f1ff42cabe890396cc6b8915",
            "bc9a1e7303f242fd97175d36cff21837",
            "026279f5c2674c46a78f0c57331b4507",
            "eab1feffc0f0470f846d70b1ec3e7a77",
            "11cce10e005d43f581089c9d80f21f72",
            "4d028ec02d8149d7ae4ebf9c59b1efb7",
            "59befed61a804d11a930d655b0df8c16",
            "e6deac6e38f844c88657d3cff09b11f9",
            "5f9dcbc02a04417f932cfb1820b0b529",
            "156ec434af7c45f48f5c7b8ac4d2bd3e",
            "981ae810520b44dfb2ab945c97fb9884"
          ]
        },
        "id": "xSMLegmvLLJe",
        "outputId": "6c090a11-4916-4195-ffcc-63cfd64f1afa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46978020f1ff42cabe890396cc6b8915"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from diffusers import DDPMScheduler, UNet2DModel\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo5cT4p6Lrr1"
      },
      "source": [
        "## The Data\n",
        "\n",
        "I give it a try with pokemon image dataset from kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path/\"pokemons\""
      ],
      "metadata": {
        "id": "um1rpLTzTktl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download pizza, steak, sushi data\n",
        "    with open(data_path / \"pokemons.zip\", \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/shani1610/learning-diffusion-models/blob/main/data/pokemons.zip\")\n",
        "        print(\"Downloading pokemon data...\")\n",
        "        f.write(request.content)\n",
        "\n",
        "    # Unzip pizza, steak, sushi data\n",
        "    with zipfile.ZipFile(data_path / \"pokemons.zip\", \"r\") as zip_ref:\n",
        "        print(\"Unzipping pokemons data...\")\n",
        "        zip_ref.extractall(image_path)"
      ],
      "metadata": {
        "id": "y6l8_J6hcLl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize an image"
      ],
      "metadata": {
        "id": "OpokARpWTsPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Set seed\n",
        "random.seed(50) # <- try changing this and see what happens\n",
        "\n",
        "# 1. Get all image paths (* means \"any combination\")\n",
        "image_path_list = list(image_path.glob(\"*/*/*.png\"))\n",
        "\n",
        "# 2. Get random image path\n",
        "random_image_path = random.choice(image_path_list)\n",
        "\n",
        "# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\n",
        "image_class = random_image_path.parent.stem\n",
        "\n",
        "# 4. Open image\n",
        "img = Image.open(random_image_path)\n",
        "\n",
        "# 5. Print metadata\n",
        "print(f\"Random image path: {random_image_path}\")\n",
        "print(f\"Image class: {image_class}\")\n",
        "print(f\"Image height: {img.height}\")\n",
        "print(f\"Image width: {img.width}\")\n",
        "img"
      ],
      "metadata": {
        "id": "g4CkCyQFTfyC",
        "outputId": "3d7a7935-6411-46d0-e6a0-4c2899f00fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random image path: pokemons/train/pokemon/alakazam.png\n",
            "Image class: pokemon\n",
            "Image height: 120\n",
            "Image width: 120\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=120x120>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforming data\n",
        "\n",
        "Before we can use our image data with PyTorch we need to:\n",
        "\n",
        "Turn it into tensors (numerical representations of our images).\n",
        "\n",
        "Turn it into a torch.utils.data.Dataset and subsequently a torch.utils.data.DataLoader, we'll call these Dataset and DataLoader for short."
      ],
      "metadata": {
        "id": "lMmrzG_3UbQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "5PlmM2UkUXyX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's write a series of transform steps that:\n",
        "\n",
        "Resize the images using transforms.Resize() (from about 120x120 to 28x28)\n",
        "\n",
        "Turn our images from a PIL image to a PyTorch tensor using transforms.ToTensor()"
      ],
      "metadata": {
        "id": "MT4CqAdRVOWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write transform for image\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(28, 28)),\n",
        "    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0\n",
        "])"
      ],
      "metadata": {
        "id": "u7JWhGvCVJva"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup train and testing paths\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "id": "sSLXQiz1VuYD",
        "outputId": "4d224124-4710-4e6e-a282-064114723d8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('pokemons/train'), PosixPath('pokemons/test'))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use ImageFolder to create dataset(s)\n",
        "from torchvision import datasets\n",
        "train_data = datasets.ImageFolder(root=train_dir, # target folder of images\n",
        "                                  transform=data_transform, # transforms to perform on data (images)\n",
        "                                  target_transform=None) # transforms to perform on labels (if necessary)\n",
        "\n",
        "test_data = datasets.ImageFolder(root=test_dir,\n",
        "                                 transform=data_transform)\n",
        "\n",
        "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")"
      ],
      "metadata": {
        "id": "1CHBdz8zVBS0",
        "outputId": "acd33bc7-c6e1-4f97-b3b9-bd3519f7f024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Found no valid file for the classes .ipynb_checkpoints. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1871653117ba>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use ImageFolder to create dataset(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train_data = datasets.ImageFolder(root=train_dir, # target folder of images\n\u001b[0m\u001b[1;32m      4\u001b[0m                                   \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# transforms to perform on data (images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                   target_transform=None) # transforms to perform on labels (if necessary)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         samples = self.make_dataset(\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;31m# is potentially overridden and thus could have a different logic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The class_to_idx parameter cannot be None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         return make_dataset(\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_empty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mextensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"Supported extensions are: {extensions if isinstance(extensions, str) else ', '.join(extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Found no valid file for the classes .ipynb_checkpoints. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn train and test Datasets into DataLoaders\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=8, # how many samples per batch?\n",
        "                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n",
        "                              shuffle=True) # shuffle the data?"
      ],
      "metadata": {
        "id": "5OylBy-HV9Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBD5PA34Lqrf"
      },
      "outputs": [],
      "source": [
        "x, y = next(iter(train_dataloader))\n",
        "print('Input shape:', x.shape)\n",
        "print('Labels:', y)\n",
        "plt.imshow(torchvision.utils.make_grid(x)[0], cmap='Greys');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q387XhREdfbI"
      },
      "source": [
        "Each image is a greyscale 28px by 28px drawing of a digit, with values ranging from 0 to 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P3m5yaAMDYk"
      },
      "source": [
        "## The Corruption Process\n",
        "\n",
        "Pretend you haven't read any diffusion model papers, but you know the process involves adding noise. How would you do it?\n",
        "\n",
        "We probably want an easy way to control the amount of corruption. So what if we take in a parameter for the `amount` of noise to add, and then we do:\n",
        "\n",
        "`noise = torch.rand_like(x)`\n",
        "\n",
        "`noisy_x =  (1-amount)*x + amount*noise`\n",
        "\n",
        "If amount = 0, we get back the input without any changes. If amount gets up to 1, we get back noise with no trace of the input x. By mixing the input with noise this way, we keep the output in the same range (0 to 1).\n",
        "\n",
        "We can implement this fairly easily (just watch the shapes so you don't get burnt by broadcasting rules):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIQ4hsd4L_1W"
      },
      "outputs": [],
      "source": [
        "def corrupt(x, amount):\n",
        "  \"\"\"Corrupt the input `x` by mixing it with noise according to `amount`\"\"\"\n",
        "  noise = torch.rand_like(x)\n",
        "  amount = amount.view(-1, 1, 1, 1) # Sort shape so broadcasting works\n",
        "  return x*(1-amount) + noise*amount"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb9jjvv2MbWy"
      },
      "source": [
        "And looking at the results visually to see that it works as expected:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crLhiM4xMRoZ"
      },
      "outputs": [],
      "source": [
        "# Plotting the input data\n",
        "fig, axs = plt.subplots(2, 1, figsize=(12, 5))\n",
        "axs[0].set_title('Input data')\n",
        "axs[0].imshow(torchvision.utils.make_grid(x)[0], cmap='Greys')\n",
        "\n",
        "# Adding noise\n",
        "amount = torch.linspace(0, 1, x.shape[0]) # Left to right -> more corruption\n",
        "noised_x = corrupt(x, amount)\n",
        "\n",
        "# Plotting the noised version\n",
        "axs[1].set_title('Corrupted data (-- amount increases -->)')\n",
        "axs[1].imshow(torchvision.utils.make_grid(noised_x)[0], cmap='Greys');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HafjRb3Md655"
      },
      "source": [
        "As noise amount approaches one, our data begins to look like pure random noise. But for most noise amounts, you can guess the digit fairly well. Do you think this is optimal?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKAzyRX6M_cE"
      },
      "source": [
        "## The Model\n",
        "\n",
        "We'd like a model that takes in a 28px noisy images and outputs a prediction of the same shape. A popular choice here is an architecture called a UNet. [Originally invented for segmentation tasks in medical imagery](https://arxiv.org/abs/1505.04597), a UNet consists of a 'constricting path' through which data is compressed down and an 'expanding path' through which it expands back up to the original dimension (similar to an autoencoder) but also features skip connections that allow for information and gradients to flow across at different levels.\n",
        "\n",
        "Some UNets feature complex blocks at each stage, but for this toy demo we'll build a minimal example that takes in a one-channel image and passes it through three convolutional layers on the down path (the down_layers in the diagram and code) and three on the up path, with skip connections between the down and up layers. We'll use max pooling for downsampling and `nn.Upsample` for upsampling rather than relying on learnable layers like more complex UNets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcUmsf9jyb9W"
      },
      "source": [
        "\n",
        "This is what that looks like in code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBvv4rUrMTzH"
      },
      "outputs": [],
      "source": [
        "class BasicUNet(nn.Module):\n",
        "    \"\"\"A minimal UNet implementation.\"\"\"\n",
        "    def __init__(self, in_channels=1, out_channels=1):\n",
        "        super().__init__()\n",
        "        self.down_layers = torch.nn.ModuleList([\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=5, padding=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
        "        ])\n",
        "        self.up_layers = torch.nn.ModuleList([\n",
        "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
        "            nn.Conv2d(64, 32, kernel_size=5, padding=2),\n",
        "            nn.Conv2d(32, out_channels, kernel_size=5, padding=2),\n",
        "        ])\n",
        "        self.act = nn.SiLU() # The activation function\n",
        "        self.downscale = nn.MaxPool2d(2)\n",
        "        self.upscale = nn.Upsample(scale_factor=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = []\n",
        "        for i, l in enumerate(self.down_layers):\n",
        "            x = self.act(l(x)) # Through the layer and the activation function\n",
        "            if i < 2: # For all but the third (final) down layer:\n",
        "              h.append(x) # Storing output for skip connection\n",
        "              x = self.downscale(x) # Downscale ready for the next layer\n",
        "\n",
        "        for i, l in enumerate(self.up_layers):\n",
        "            if i > 0: # For all except the first up layer\n",
        "              x = self.upscale(x) # Upscale\n",
        "              x += h.pop() # Fetching stored output (skip connection)\n",
        "            x = self.act(l(x)) # Through the layer and the activation function\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul24wibkqKF8"
      },
      "source": [
        "We can verify that the output shape is the same as the input, as we expect:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQAia44ONvy3"
      },
      "outputs": [],
      "source": [
        "net = BasicUNet()\n",
        "x = torch.rand(8, 1, 28, 28)\n",
        "net(x).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRMIxRWjqOve"
      },
      "source": [
        "This network has just over 300,000 parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "511xdpCSTyy_"
      },
      "outputs": [],
      "source": [
        "sum([p.numel() for p in net.parameters()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJUnmxSAqSwW"
      },
      "source": [
        "You can explore changing the number of channels in each layer or swapping in different architectures if you want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ard-UrsmNxDn"
      },
      "source": [
        "## Training the network\n",
        "\n",
        "So what should the model do, exactly? Again, there are various takes on this but for this demo let's pick a simple framing: given a corrupted input noisy_x the model should output its best guess for what the original x looks like. We will compare this to the actual value via the mean squared error.\n",
        "\n",
        "We can now have a go at training the network.\n",
        "- Get a batch of data\n",
        "- Corrupt it by random amounts\n",
        "- Feed it through the model\n",
        "- Compare the model predictions with the clean images to calculate our loss\n",
        "- Update the model's parameters accordingly.\n",
        "\n",
        "Feel free to modify this and see if you can get it working better!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MW0xsLGNrXL"
      },
      "outputs": [],
      "source": [
        "# Dataloader (you can mess with batch size)\n",
        "batch_size = 128\n",
        "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# How many runs through the data should we do?\n",
        "n_epochs = 3\n",
        "\n",
        "# Create the network\n",
        "net = BasicUNet()\n",
        "net.to(device)\n",
        "\n",
        "# Our loss function\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# The optimizer\n",
        "opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "# Keeping a record of the losses for later viewing\n",
        "losses = []\n",
        "\n",
        "# The training loop\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    for x, y in train_dataloader:\n",
        "\n",
        "        # Get some data and prepare the corrupted version\n",
        "        x = x.to(device) # Data on the GPU\n",
        "        noise_amount = torch.rand(x.shape[0]).to(device) # Pick random noise amounts\n",
        "        noisy_x = corrupt(x, noise_amount) # Create our noisy x\n",
        "\n",
        "        # Get the model prediction\n",
        "        pred = net(noisy_x)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_fn(pred, x) # How close is the output to the true 'clean' x?\n",
        "\n",
        "        # Backprop and update the params:\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        # Store the loss for later\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # Print our the average of the loss values for this epoch:\n",
        "    avg_loss = sum(losses[-len(train_dataloader):])/len(train_dataloader)\n",
        "    print(f'Finished epoch {epoch}. Average loss for this epoch: {avg_loss:05f}')\n",
        "\n",
        "# View the loss curve\n",
        "plt.plot(losses)\n",
        "plt.ylim(0, 0.1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3YqLVvzOn-D"
      },
      "source": [
        "We can try to see what the model predictions look like by grabbing a batch of data, corrupting it by different amounts and then seeing the models predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Qm6JowixOpH9"
      },
      "outputs": [],
      "source": [
        "#@markdown Visualizing model predictions on noisy inputs:\n",
        "\n",
        "# Fetch some data\n",
        "x, y = next(iter(train_dataloader))\n",
        "x = x[:8] # Only using the first 8 for easy plotting\n",
        "\n",
        "# Corrupt with a range of amounts\n",
        "amount = torch.linspace(0, 1, x.shape[0]) # Left to right -> more corruption\n",
        "noised_x = corrupt(x, amount)\n",
        "\n",
        "# Get the model predictions\n",
        "with torch.no_grad():\n",
        "  preds = net(noised_x.to(device)).detach().cpu()\n",
        "\n",
        "# Plot\n",
        "fig, axs = plt.subplots(3, 1, figsize=(12, 7))\n",
        "axs[0].set_title('Input data')\n",
        "axs[0].imshow(torchvision.utils.make_grid(x)[0].clip(0, 1), cmap='Greys')\n",
        "axs[1].set_title('Corrupted data')\n",
        "axs[1].imshow(torchvision.utils.make_grid(noised_x)[0].clip(0, 1), cmap='Greys')\n",
        "axs[2].set_title('Network Predictions')\n",
        "axs[2].imshow(torchvision.utils.make_grid(preds)[0].clip(0, 1), cmap='Greys');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hagyHC2JO4fh"
      },
      "source": [
        "You can see that for the lower amounts the predictions are pretty good! But as the level gets very high there is less for the model to work with, and by the time we get to amount=1 it outputs a blurry mess close to the mean of the dataset to try and hedge its bets on what the output might look like..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulBCCIS1PN-u"
      },
      "source": [
        "## Sampling\n",
        "\n",
        "If our predictions at high noise levels aren't very good, how do we generate images?\n",
        "\n",
        "Well, what if we start from random noise, look at the model predictions but then only move a small amount towards that prediction - say, 20% of the way there. Now we have a very noisy image in which there is perhaps a hint of structure, which we can feed into the model to get a new prediction. The hope is that this new prediction is slightly better than the first one (since our starting point is slightly less noisy) and so we can take another small step with this new, better prediction.\n",
        "\n",
        "Repeat a few times and (if all goes well) we get an image out! Here is that process illustrated over just 5 steps, visualizing the model input (left) and the predicted denoised images (right) at each stage. Note that even though the model predicts the denoised image even at step 1, we only move x part of the way there. Over a few steps the structures appear and are refined, until we get our final outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vphaFabJPPKo"
      },
      "outputs": [],
      "source": [
        "#@markdown Sampling strategy: Break the process into 5 steps and move 1/5'th of the way there each time:\n",
        "n_steps = 5\n",
        "x = torch.rand(8, 1, 28, 28).to(device) # Start from random\n",
        "step_history = [x.detach().cpu()]\n",
        "pred_output_history = []\n",
        "\n",
        "for i in range(n_steps):\n",
        "    with torch.no_grad(): # No need to track gradients during inference\n",
        "        pred = net(x) # Predict the denoised x0\n",
        "    pred_output_history.append(pred.detach().cpu()) # Store model output for plotting\n",
        "    mix_factor = 1/(n_steps - i) # How much we move towards the prediction\n",
        "    x = x*(1-mix_factor) + pred*mix_factor # Move part of the way there\n",
        "    step_history.append(x.detach().cpu()) # Store step for plotting\n",
        "\n",
        "fig, axs = plt.subplots(n_steps, 2, figsize=(9, 4), sharex=True)\n",
        "axs[0,0].set_title('x (model input)')\n",
        "axs[0,1].set_title('model prediction')\n",
        "for i in range(n_steps):\n",
        "    axs[i, 0].imshow(torchvision.utils.make_grid(step_history[i])[0].clip(0, 1), cmap='Greys')\n",
        "    axs[i, 1].imshow(torchvision.utils.make_grid(pred_output_history[i])[0].clip(0, 1), cmap='Greys')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJzK7NrB7U-E"
      },
      "source": [
        "We can split the process up into more steps, and hope for better images that way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5yLqqeIhOxUs"
      },
      "outputs": [],
      "source": [
        "#@markdown Showing more results, using 40 sampling steps\n",
        "n_steps = 40\n",
        "x = torch.rand(64, 1, 28, 28).to(device)\n",
        "for i in range(n_steps):\n",
        "  noise_amount = torch.ones((x.shape[0], )).to(device) * (1-(i/n_steps)) # Starting high going low\n",
        "  with torch.no_grad():\n",
        "    pred = net(x)\n",
        "  mix_factor = 1/(n_steps - i)\n",
        "  x = x*(1-mix_factor) + pred*mix_factor\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
        "ax.imshow(torchvision.utils.make_grid(x.detach().cpu(), nrow=8)[0].clip(0, 1), cmap='Greys')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7oPvQ55uE1QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo1OsL6y568K"
      },
      "source": [
        "Not great, but there are some recognizable digits there! You can experiment with training for longer (say, 10 or 20 epochs) and tweaking model config, learning rate, optimizer and so on. Also, don't forget that fashionMNIST is a one-line replacement if you want a slightly harder dataset to try."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNe55KwUSiTW"
      },
      "source": [
        "## Comparison To DDPM\n",
        "\n",
        "In this section we'll take a look at how our toy implementation differs from the approach used in the other notebook ([Introduction to Diffusers](https://github.com/huggingface/diffusion-models-class/blob/main/unit1/01_introduction_to_diffusers.ipynb)), which is based on the DDPM paper.\n",
        "\n",
        "We'll see that\n",
        "\n",
        "\n",
        "*   The diffusers `UNet2DModel` is a bit more advanced than our BasicUNet\n",
        "*   The corruption process in handled differently\n",
        "*   The training objective is different, involving predicting the noise rather than the denoised image\n",
        "*   The model is conditioned on the amount of noise present via timestep conditioning, where t is passed as an additional argument to the forward method.\n",
        "*   There are a number of different sampling strategies available, which should work better than our simplistic version above.\n",
        "\n",
        "There have been a number of improvements suggested since the DDPM paper came out, but this example is hopefully instructive as to the different available design decisions. Once you've read through this, you may enjoy diving into the paper ['Elucidating the Design Space of Diffusion-Based Generative Models'](https://arxiv.org/abs/2206.00364) which explores all of these components in some detail and makes new recommendations for how to get the best performance.\n",
        "\n",
        "If all of this is too technical or intimidating, don't worry! Feel free to skip the rest of this notebook or save it for a rainy day.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li3R8czNUPTe"
      },
      "source": [
        "### The UNet\n",
        "\n",
        "The diffusers UNet2DModel model has a number of improvements over our basic UNet above:\n",
        "\n",
        "*   GroupNorm applies group normalization to the inputs of each block\n",
        "*   Dropout layers for smoother training\n",
        "*   Multiple resnet layers per block (if layers_per_block isn't set to 1)\n",
        "*   Attention (usually used only at lower resolution blocks)\n",
        "*   Conditioning on the timestep.\n",
        "*   Downsampling and upsampling blocks with learnable parameters\n",
        "\n",
        "Let's create and inspect a UNet2DModel:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWOGYGj3U7tR"
      },
      "outputs": [],
      "source": [
        "model = UNet2DModel(\n",
        "    sample_size=28,           # the target image resolution\n",
        "    in_channels=1,            # the number of input channels, 3 for RGB images\n",
        "    out_channels=1,           # the number of output channels\n",
        "    layers_per_block=2,       # how many ResNet layers to use per UNet block\n",
        "    block_out_channels=(32, 64, 64), # Roughly matching our basic unet example\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",        # a regular ResNet downsampling block\n",
        "        \"AttnDownBlock2D\",    # a ResNet downsampling block with spatial self-attention\n",
        "        \"AttnDownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"AttnUpBlock2D\",\n",
        "        \"AttnUpBlock2D\",      # a ResNet upsampling block with spatial self-attention\n",
        "        \"UpBlock2D\",          # a regular ResNet upsampling block\n",
        "      ),\n",
        ")\n",
        "#print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJVstuWVWRxx"
      },
      "source": [
        "As you can see, a little more going on! It also has significantly more parameters than our BasicUNet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-qwAZhsQd0x"
      },
      "outputs": [],
      "source": [
        "sum([p.numel() for p in model.parameters()]) # 1.7M vs the ~309k parameters of the BasicUNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDYSTW5pWXVc"
      },
      "source": [
        "We can replicate the training shown above using this model in place of our original one. We need to pass both x and timestep to the model (here I always pass t=0 to show that it works without this timestep conditioning and to keep the sampling code easy, but you can also try feeding in `(amount*1000)` to get a timestep equivalent from the corruption amount). Lines changed are shown with `#<<<` if you want to inspect the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "y4MMUiUiT1LZ"
      },
      "outputs": [],
      "source": [
        "#@markdown Trying UNet2DModel instead of BasicUNet:\n",
        "\n",
        "# Dataloader (you can mess with batch size)\n",
        "batch_size = 128\n",
        "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# How many runs through the data should we do?\n",
        "n_epochs = 3\n",
        "\n",
        "# Create the network\n",
        "net = UNet2DModel(\n",
        "    sample_size=28,  # the target image resolution\n",
        "    in_channels=1,  # the number of input channels, 3 for RGB images\n",
        "    out_channels=1,  # the number of output channels\n",
        "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
        "    block_out_channels=(32, 64, 64),  # Roughly matching our basic unet example\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
        "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
        "        \"AttnDownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"AttnUpBlock2D\",\n",
        "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
        "        \"UpBlock2D\",   # a regular ResNet upsampling block\n",
        "      ),\n",
        ") #<<<\n",
        "net.to(device)\n",
        "\n",
        "# Our loss finction\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# The optimizer\n",
        "opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "# Keeping a record of the losses for later viewing\n",
        "losses = []\n",
        "\n",
        "# The training loop\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    for x, y in train_dataloader:\n",
        "\n",
        "        # Get some data and prepare the corrupted version\n",
        "        x = x.to(device) # Data on the GPU\n",
        "        noise_amount = torch.rand(x.shape[0]).to(device) # Pick random noise amounts\n",
        "        noisy_x = corrupt(x, noise_amount) # Create our noisy x\n",
        "\n",
        "        # Get the model prediction\n",
        "        pred = net(noisy_x, 0).sample #<<< Using timestep 0 always, adding .sample\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = loss_fn(pred, x) # How close is the output to the true 'clean' x?\n",
        "\n",
        "        # Backprop and update the params:\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        # Store the loss for later\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # Print our the average of the loss values for this epoch:\n",
        "    avg_loss = sum(losses[-len(train_dataloader):])/len(train_dataloader)\n",
        "    print(f'Finished epoch {epoch}. Average loss for this epoch: {avg_loss:05f}')\n",
        "\n",
        "# Plot losses and some samples\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Losses\n",
        "axs[0].plot(losses)\n",
        "axs[0].set_ylim(0, 0.1)\n",
        "axs[0].set_title('Loss over time')\n",
        "\n",
        "# Samples\n",
        "n_steps = 40\n",
        "x = torch.rand(64, 1, 28, 28).to(device)\n",
        "for i in range(n_steps):\n",
        "  noise_amount = torch.ones((x.shape[0], )).to(device) * (1-(i/n_steps)) # Starting high going low\n",
        "  with torch.no_grad():\n",
        "    pred = net(x, 0).sample\n",
        "  mix_factor = 1/(n_steps - i)\n",
        "  x = x*(1-mix_factor) + pred*mix_factor\n",
        "\n",
        "axs[1].imshow(torchvision.utils.make_grid(x.detach().cpu(), nrow=8)[0].clip(0, 1), cmap='Greys')\n",
        "axs[1].set_title('Generated Samples');"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1mr5W7P-Ybhq"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46978020f1ff42cabe890396cc6b8915": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc9a1e7303f242fd97175d36cff21837",
              "IPY_MODEL_026279f5c2674c46a78f0c57331b4507",
              "IPY_MODEL_eab1feffc0f0470f846d70b1ec3e7a77"
            ],
            "layout": "IPY_MODEL_11cce10e005d43f581089c9d80f21f72"
          }
        },
        "bc9a1e7303f242fd97175d36cff21837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d028ec02d8149d7ae4ebf9c59b1efb7",
            "placeholder": "",
            "style": "IPY_MODEL_59befed61a804d11a930d655b0df8c16",
            "value": ""
          }
        },
        "026279f5c2674c46a78f0c57331b4507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6deac6e38f844c88657d3cff09b11f9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f9dcbc02a04417f932cfb1820b0b529",
            "value": 0
          }
        },
        "eab1feffc0f0470f846d70b1ec3e7a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_156ec434af7c45f48f5c7b8ac4d2bd3e",
            "placeholder": "",
            "style": "IPY_MODEL_981ae810520b44dfb2ab945c97fb9884",
            "value": "0/0[00:00&lt;?,?it/s]"
          }
        },
        "11cce10e005d43f581089c9d80f21f72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d028ec02d8149d7ae4ebf9c59b1efb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59befed61a804d11a930d655b0df8c16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6deac6e38f844c88657d3cff09b11f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5f9dcbc02a04417f932cfb1820b0b529": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "156ec434af7c45f48f5c7b8ac4d2bd3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "981ae810520b44dfb2ab945c97fb9884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}