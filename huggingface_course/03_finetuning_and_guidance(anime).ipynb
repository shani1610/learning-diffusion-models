{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shani1610/learning-diffusion-models/blob/main/huggingface_course/03_finetuning_and_guidance(anime).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r1FWCijiq4S"
      },
      "source": [
        "# Fine-Tuning and Guidance\n",
        "\n",
        "In this notebook, we're going to cover two main approaches for adapting existing diffusion models:\n",
        "\n",
        "* With **fine-tuning**, we'll re-train existing models on new data to change the type of output they produce\n",
        "* With **guidance**, we'll take an existing model and steer the generation process at inference time for additional control\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQnpiylwiq4V"
      },
      "source": [
        "## Setup and Imports\n",
        "\n",
        "To save your fine-tuned models to the Hugging Face Hub, you'll need to login with a **token that has write access**. The code below will prompt you for this and link to the relevant tokens page of your account. You'll also need a Weights and Biases account if you'd like to use the training script to log samples as the model trains - again, the code should prompt you to sign in where needed.\n",
        "\n",
        "Apart from that, the only set-up is installing a few dependencies, importing everything we'll need and specifying which device we'll use:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwZ2wq0kiq4W"
      },
      "outputs": [],
      "source": [
        "%pip install -qq diffusers datasets accelerate wandb open-clip-torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLn_hyW1iq4X"
      },
      "outputs": [],
      "source": [
        "# Code to log in to the Hugging Face Hub, needed for sharing models\n",
        "# Make sure you use a token with WRITE access\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oITY8Xuoiq4Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from datasets import load_dataset\n",
        "from diffusers import DDIMScheduler, DDPMPipeline\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcWEkgu1iq4Y"
      },
      "outputs": [],
      "source": [
        "device = (\n",
        "    \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGbcWra5iq4Z"
      },
      "source": [
        "## Loading A Pre-Trained Pipeline\n",
        "\n",
        "To begin this notebook, let's load an existing pipeline and see what we can do with it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvuy4g9viq4Z"
      },
      "outputs": [],
      "source": [
        "image_pipe = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\")\n",
        "image_pipe.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4w9lp4ciq4a"
      },
      "source": [
        "## Faster Sampling with DDIM\n",
        "\n",
        "At every step, the model is fed a noisy input and asked to predict the noise (and thus an estimate of what the fully denoised image might look like). Initially these predictions are not very good, which is why we break the process down into many steps. However, using 1000+ steps has been found to be unnecessary, and a flurry of recent research has explored how to achieve good samples with as few steps as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tp8bC2eiq4b"
      },
      "outputs": [],
      "source": [
        "# Create new scheduler and set num inference steps\n",
        "scheduler = DDIMScheduler.from_pretrained(\"google/ddpm-celebahq-256\")\n",
        "scheduler.set_timesteps(num_inference_steps=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4ZWG_kOiq4b"
      },
      "source": [
        "You can see that this model does 40 steps total, each jumping the equivalent of 25 steps of the original 1000-step schedule:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHSq_wmxiq4b"
      },
      "outputs": [],
      "source": [
        "scheduler.timesteps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1E_jTkbiq4c"
      },
      "source": [
        "As you can see, the initial predictions are not great but as the process goes on the predicted outputs get more and more refined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTYgNpXkiq4c"
      },
      "source": [
        "Alright - we can get samples in a reasonable time now! This should speed things up as we move through the rest of this notebook :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SfF2qQsiq4d"
      },
      "source": [
        "## Fine-Tuning\n",
        "\n",
        "Now for the fun bit! Given this pre-trained pipeline, how might we re-train the model to generate images based on new training data?\n",
        "\n",
        "It turns out that this looks nearly identical to training a model from scratch (as we saw in [Unit 1](https://github.com/huggingface/diffusion-models-class/tree/main/unit1)) except that we begin with the existing model. Let's see this in action and talk about a few additional considerations as we go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvUuPaLiiq4d"
      },
      "source": [
        "First, the dataset: I will try with try [these anime faces](https://huggingface.co/datasets/huggan/anime-faces) for something closer to the original training data of this faces model. Run the code below to download the butterflies dataset and create a dataloader we can sample a batch of images from:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmP9US0Eiq4d"
      },
      "outputs": [],
      "source": [
        "# @markdown load and prepare a dataset:\n",
        "# Not on Colab? Comments with #@ enable UI tweaks like headings or user inputs\n",
        "# but can safely be ignored if you're working on a different platform.\n",
        "\n",
        "dataset_name = \"huggan/anime-faces\"  # @param\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "image_size = 65  # @param\n",
        "batch_size = 4  # @param\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def transform(examples):\n",
        "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
        "    return {\"images\": images}\n",
        "\n",
        "\n",
        "dataset.set_transform(transform)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "\n",
        "print(\"Previewing batch:\")\n",
        "batch = next(iter(train_dataloader))\n",
        "grid = torchvision.utils.make_grid(batch[\"images\"], nrow=4)\n",
        "plt.imshow(grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiqPNmroiq4d"
      },
      "source": [
        "**Consideration 1:** our batch size here (4) is pretty small, since we're training at large image size (256px) using a fairly large model and we'll run out of GPU RAM if we push the batch size too high. You can reduce the image size to speed things up and allow for larger batches, but these models were designed and originally trained for 256px generation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Anime-Faces dataset the images are 64x64, so lets see what kind of adujustions it will require from us."
      ],
      "metadata": {
        "id": "P2VLzwlZtoBT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oisTAB8siq4d"
      },
      "source": [
        "Now for the training loop. We'll update the weights of the pre-trained model by setting the optimization target to `image_pipe.unet.parameters()`. The rest is nearly identical to the example training loop from Unit 1. This takes about 10 minutes to run on Colab, so now is a good time to grab a coffee of tea while you wait:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r3mx0dFiq4d"
      },
      "outputs": [],
      "source": [
        "num_epochs = 2  # @param\n",
        "lr = 1e-5  # 2param\n",
        "grad_accumulation_steps = 2  # @param\n",
        "\n",
        "optimizer = torch.optim.AdamW(image_pipe.unet.parameters(), lr=lr)\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
        "        clean_images = batch[\"images\"].to(device)\n",
        "        # Sample noise to add to the images\n",
        "        noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
        "        bs = clean_images.shape[0]\n",
        "\n",
        "        # Sample a random timestep for each image\n",
        "        timesteps = torch.randint(\n",
        "            0,\n",
        "            image_pipe.scheduler.num_train_timesteps,\n",
        "            (bs,),\n",
        "            device=clean_images.device,\n",
        "        ).long()\n",
        "\n",
        "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
        "        # (this is the forward diffusion process)\n",
        "        noisy_images = image_pipe.scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "        # Get the model prediction for the noise\n",
        "        noise_pred = image_pipe.unet(noisy_images, timesteps, return_dict=False)[0]\n",
        "\n",
        "        # Compare the prediction with the actual noise:\n",
        "        loss = F.mse_loss(\n",
        "            noise_pred, noise\n",
        "        )  # NB - trying to predict noise (eps) not (noisy_ims-clean_ims) or just (clean_ims)\n",
        "\n",
        "        # Store for later plotting\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Update the model parameters with the optimizer based on this loss\n",
        "        loss.backward(loss)\n",
        "\n",
        "        # Gradient accumulation:\n",
        "        if (step + 1) % grad_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch} average loss: {sum(losses[-len(train_dataloader):])/len(train_dataloader)}\"\n",
        "    )\n",
        "\n",
        "# Plot the loss curve:\n",
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atfjfzXWiq4e"
      },
      "source": [
        "**Consideration 2:** Our loss signal is extremely noisy, since we're only working with four examples at random noise levels for each step. This is not ideal for training. One fix is to use an extremely low learning rate to limit the size of the update each step. It would be even better if we could find some way to get the same benefit we would get from using a larger batch size _without_ the memory requirements skyrocketing...\n",
        "\n",
        "Enter [gradient accumulation](https://kozodoi.me/python/deep%20learning/pytorch/tutorial/2021/02/19/gradient-accumulation.html#:~:text=Simply%20speaking%2C%20gradient%20accumulation%20means,might%20find%20this%20tutorial%20useful.). If we call `loss.backward()` multiple times before running `optimizer.step()` and `optimizer.zero_grad()`, then PyTorch accumulates (sums) the gradients, effectively merging the signal from several batches to give a single (better) estimate which is then used to update the parameters. This results in fewer total updates being made, just like we'd see if we used a larger batch size. This is something many frameworks will handle for you (for example, [ðŸ¤— Accelerate makes this easy](https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation)) but it is nice to see it implemented from scratch since this is a useful technique for dealing with training under GPU memory constraints! As you can see from the code above (after the `# Gradient accumulation` comment) there really isn't much code needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1S0j5V4iq4e"
      },
      "outputs": [],
      "source": [
        "# Exercise: See if you can add gradient accumulation to the training loop in Unit 1.\n",
        "# How does it perform? Think how you might adjust the learning rate based on the\n",
        "# number of gradient accumulation steps - should it stay the same as before?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JstyL8hBiq4f"
      },
      "source": [
        "It's fun to see how the generated samples change as training progresses - even though the loss doesn't appear to be improving much, we can see a progression away from the original domain (images of bedrooms) towards the new training data (wikiart). At the end of this notebook is commented-out code for fine-tuning a model using this script as an alternative to running the cell above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEiDc1XGiq4f"
      },
      "outputs": [],
      "source": [
        "# Exercise: see if you can modify the official example training script we saw\n",
        "# in Unit 1 to begin with a pre-trained model rather than training from scratch.\n",
        "# Compare it to the minimal script linked above - what extra features is the minimal script missing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gjxgZmiq4f"
      },
      "source": [
        "Generating some images with this model, we can see that these faces are already looking mighty strange!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbvIwjh6iq4f"
      },
      "outputs": [],
      "source": [
        "# @markdown Generate and plot some images:\n",
        "x = torch.randn(8, 3, 64, 64).to(device)  # Batch of 8\n",
        "for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "    model_input = scheduler.scale_model_input(x, t)\n",
        "    with torch.no_grad():\n",
        "        noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n",
        "    x = scheduler.step(noise_pred, t, x).prev_sample\n",
        "grid = torchvision.utils.make_grid(x, nrow=4)\n",
        "plt.imshow(grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eprrBRkPiq4g"
      },
      "source": [
        "**Consideration 4:** Fine-tuning can be quite unpredictable! If we trained for a lot longer, we might see some perfect butterflies. But the intermediate steps can be extremely interesting in their own right, especially if your interests are more towards the artistic side! Explore training for very short or very long periods of time, and varying the learning rate to see how this affects the kinds of output the final model produces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrW-VnCUiq4g"
      },
      "source": [
        "### Code for fine-tuning a model using the minimal example script we used on the WikiArt demo model\n",
        "\n",
        "If you'd like to train a similar model to the one I made on WikiArt, you can uncomment and run the cells below. Since this takes a while and may exhaust your GPU memory, I recommend doing this _after_ working through the rest of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR6--hjwiq4k"
      },
      "outputs": [],
      "source": [
        "## To download the fine-tuning script:\n",
        "# !wget https://github.com/huggingface/diffusion-models-class/raw/main/unit2/finetune_model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt-wzTKdiq4k"
      },
      "outputs": [],
      "source": [
        "## To run the script, training the face model on some vintage faces\n",
        "## (ideally run this in a terminal):\n",
        "# !python finetune_model.py --image_size 128 --batch_size 8 --num_epochs 16\\\n",
        "#     --grad_accumulation_steps 2 --start_model \"google/ddpm-celebahq-256\"\\\n",
        "#     --dataset_name \"Norod78/Vintage-Faces-FFHQAligned\" --wandb_project 'dm-finetune'\\\n",
        "#     --log_samples_every 100 --save_model_every 1000 --model_save_name 'vintageface'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho_ZW7GSiq4k"
      },
      "source": [
        "### Saving and Loading Fine-Tuned Pipelines\n",
        "\n",
        "Now that we've fine-tuned the U-Net in our diffusion model, let's save it to a local folder by running:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB8wJr0_iq4k"
      },
      "outputs": [],
      "source": [
        "image_pipe.save_pretrained(\"my-finetuned-model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCTX84dziq4k"
      },
      "source": [
        "As we saw in Unit 1, this will save the config, model, scheduler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIyooiU7iq4k"
      },
      "outputs": [],
      "source": [
        "!ls {\"my-finetuned-model\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnTkZ0pdiq4l"
      },
      "source": [
        "Next, you can follow the same steps outlined in Unit 1's [Introduction to Diffusers](https://github.com/huggingface/diffusion-models-class/blob/main/unit1/01_introduction_to_diffusers.ipynb) to push the model to the Hub for later use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9Kd55SPiq4l"
      },
      "outputs": [],
      "source": [
        "# @title Upload a locally saved pipeline to the hub\n",
        "\n",
        "# Code to upload a pipeline saved locally to the hub\n",
        "from huggingface_hub import HfApi, ModelCard, create_repo, get_full_repo_name\n",
        "\n",
        "# Set up repo and upload files\n",
        "model_name = \"ddpm-celebahq-finetuned-butterflies-2epochs\"  # @param What you want it called on the hub\n",
        "local_folder_name = \"my-finetuned-model\"  # @param Created by the script or one you created via image_pipe.save_pretrained('save_name')\n",
        "description = \"Describe your model here\"  # @param\n",
        "hub_model_id = get_full_repo_name(model_name)\n",
        "create_repo(hub_model_id)\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=f\"{local_folder_name}/scheduler\", path_in_repo=\"\", repo_id=hub_model_id\n",
        ")\n",
        "api.upload_folder(\n",
        "    folder_path=f\"{local_folder_name}/unet\", path_in_repo=\"\", repo_id=hub_model_id\n",
        ")\n",
        "api.upload_file(\n",
        "    path_or_fileobj=f\"{local_folder_name}/model_index.json\",\n",
        "    path_in_repo=\"model_index.json\",\n",
        "    repo_id=hub_model_id,\n",
        ")\n",
        "\n",
        "# Add a model card (optional but nice!)\n",
        "content = f\"\"\"\n",
        "---\n",
        "license: mit\n",
        "tags:\n",
        "- pytorch\n",
        "- diffusers\n",
        "- unconditional-image-generation\n",
        "- diffusion-models-class\n",
        "---\n",
        "\n",
        "# Example Fine-Tuned Model for Unit 2 of the [Diffusion Models Class ðŸ§¨](https://github.com/huggingface/diffusion-models-class)\n",
        "\n",
        "{description}\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from diffusers import DDPMPipeline\n",
        "\n",
        "pipeline = DDPMPipeline.from_pretrained('{hub_model_id}')\n",
        "image = pipeline().images[0]\n",
        "image\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "card = ModelCard(content)\n",
        "card.push_to_hub(hub_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihR3mtzKiq4l"
      },
      "source": [
        "Congratulations, you've now fine-tuned your first diffusion model!\n",
        "\n",
        "For the rest of this notebook we'll use a [model](https://huggingface.co/johnowhitaker/sd-class-wikiart-from-bedrooms) I fine-tuned from [this model trained on LSUN bedrooms](https://huggingface.co/google/ddpm-bedroom-256) approximately one epoch on the [WikiArt dataset](https://huggingface.co/datasets/huggan/wikiart). If you'd prefer, you can skip this cell and use the faces/butterflies pipeline we fine-tuned in the previous section or load one from the Hub instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aYFasy5iq4l"
      },
      "outputs": [],
      "source": [
        "# Load the pretrained pipeline\n",
        "pipeline_name = \"johnowhitaker/sd-class-wikiart-from-bedrooms\"\n",
        "image_pipe = DDPMPipeline.from_pretrained(pipeline_name).to(device)\n",
        "\n",
        "# Sample some images with a DDIM Scheduler over 40 steps\n",
        "scheduler = DDIMScheduler.from_pretrained(pipeline_name)\n",
        "scheduler.set_timesteps(num_inference_steps=40)\n",
        "\n",
        "# Random starting point (batch of 8 images)\n",
        "x = torch.randn(8, 3, 256, 256).to(device)\n",
        "\n",
        "# Minimal sampling loop\n",
        "for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "    model_input = scheduler.scale_model_input(x, t)\n",
        "    with torch.no_grad():\n",
        "        noise_pred = image_pipe.unet(model_input, t)[\"sample\"]\n",
        "    x = scheduler.step(noise_pred, t, x).prev_sample\n",
        "\n",
        "# View the results\n",
        "grid = torchvision.utils.make_grid(x, nrow=4)\n",
        "plt.imshow(grid.permute(1, 2, 0).cpu().clip(-1, 1) * 0.5 + 0.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjjsDzcSiq4l"
      },
      "source": [
        "**Consideration 5:** It is often hard to tell how well fine-tuning is working, and what 'good performance' means may vary by use-case. For example, if you're fine-tuning a text-conditioned model like stable diffusion on a small dataset you probably want it to **retain** most of its original training so that it can understand arbitrary prompts not covered by your new dataset, while **adapting** to better match the style of your new training data. This could mean using a low learning rate alongside something like exponential model averaging, as demonstrated [in this great blog post about creating a pokemon version of stable diffusion](https://lambdalabs.com/blog/how-to-fine-tune-stable-diffusion-how-we-made-the-text-to-pokemon-model-at-lambda). In a different situation, you may want to completely re-train a model on new data (such as our bedroom -> wikiart example) in which case a larger learning rate and more training makes sense. Even though the [loss plot](https://wandb.ai/johnowhitaker/dm_finetune/runs/2upaa341) is not showing much improvement, the samples clearly show a move away from the original data and towards more 'artsy' outputs, although they remain mostly incoherent.\n",
        "\n",
        "Which leads us to a the next section, as we examine how we might add additional guidance to such a model for better control over the outputs..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb1D_5Aaiq4u"
      },
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "We've covered a lot in this notebook! Let's recap the core ideas:\n",
        "- It's relatively easy to load in existing models and sample them with different schedulers\n",
        "- Fine-tuning looks just like training from scratch, except that by starting from an existing model we hope to get better results more quickly\n",
        "- To fine-tune large models on big images, we can use tricks like gradient accumulation to get around batch size limitations\n",
        "- Logging sample images is important for fine-tuning, where a loss curve might not show much useful information\n",
        "\n",
        "To put this into practice, here are some specific next steps you can take:\n",
        "- Fine-tune your own model and push it to the hub. This will involve picking a starting point (e.g. a model trained on [faces](https://huggingface.co/google/ddpm-celebahq-256), [bedrooms](https://huggingface.co/fusing/ddpm-lsun-bedroom), [cats](https://huggingface.co/fusing/ddpm-lsun-cat) or the [wikiart example above](https://huggingface.co/johnowhitaker/sd-class-wikiart-from-bedrooms)) and a dataset (perhaps these [animal faces](https://huggingface.co/datasets/huggan/AFHQv2) or your own images) and then running either the code in this notebook or the example script (demo usage below).\n",
        "- Explore guidance using your fine-tuned model, either using one of the example guidance functions (color_loss or CLIP) or inventing your own.\n",
        "- Share a demo based on this using Gradio, either modifying the [example space](https://huggingface.co/spaces/johnowhitaker/color-guided-wikiart-diffusion) to use your own model or creating your own custom version with more functionality.\n",
        "\n",
        "We look forward to seeing your results on Discord, Twitter, and elsewhere ðŸ¤—!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}